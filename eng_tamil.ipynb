{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eng-tamil.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyZhjJ6UoGSUW9GwzFFSRP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3JnkpLLRPl",
        "outputId": "868ccf48-bab7-40ec-9c8a-a680b3bf3380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://www.manythings.org/anki/tam-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-31 15:26:35--  https://www.manythings.org/anki/tam-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 172.67.173.198, 104.24.109.196, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9823 (9.6K) [application/zip]\n",
            "Saving to: ‘tam-eng.zip’\n",
            "\n",
            "\rtam-eng.zip           0%[                    ]       0  --.-KB/s               \rtam-eng.zip         100%[===================>]   9.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-31 15:26:35 (85.3 MB/s) - ‘tam-eng.zip’ saved [9823/9823]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_wC7nCoLY8H",
        "outputId": "53801c03-9275-40e8-e2e0-f6c7f64e001f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWU69gccLnZd",
        "outputId": "c03f6258-1a58-4c83-c773-4705b131be48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.listdir('/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['media',\n",
              " 'var',\n",
              " 'dev',\n",
              " 'tmp',\n",
              " 'sys',\n",
              " 'home',\n",
              " 'sbin',\n",
              " 'mnt',\n",
              " 'lib64',\n",
              " 'run',\n",
              " 'bin',\n",
              " 'usr',\n",
              " 'opt',\n",
              " 'boot',\n",
              " 'srv',\n",
              " 'etc',\n",
              " 'root',\n",
              " 'proc',\n",
              " 'lib',\n",
              " 'index.html',\n",
              " 'content',\n",
              " 'tam-eng.zip',\n",
              " 'data',\n",
              " '.dockerenv',\n",
              " 'datalab',\n",
              " 'tools',\n",
              " 'swift',\n",
              " 'tensorflow-1.15.2',\n",
              " 'lib32']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmqUzVxbEtiY"
      },
      "source": [
        "import subprocess\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('-s', help='ISO 639-3 code for a source language')\n",
        "# parser.add_argument('-t', help='ISO 639-3 code for a target language')\n",
        "# parser.add_argument('-data_dir', default='./data', help='Data directory')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = parser.parse_args()\n",
        "    source_lang = 'eng'\n",
        "    target_lang = 'tam'\n",
        "    data_dir = '/content/data/'\n",
        "\n",
        "    # download if sentences are not here\n",
        "    sentences_path = os.path.join(data_dir, 'sentences.csv')\n",
        "    if not os.path.isfile(sentences_path):\n",
        "        if not os.path.isfile(os.path.join(data_dir, 'sentences.tar.bz2')):\n",
        "            subprocess.run(\n",
        "                \"wget https://downloads.tatoeba.org/exports/sentences.tar.bz2 -P \" + data_dir,\n",
        "                shell=True)\n",
        "\n",
        "        subprocess.run(\n",
        "            \"tar xvjC {0} -f {0}/sentences.tar.bz2\".format(data_dir), shell=True)\n",
        "\n",
        "    # download if links are not here\n",
        "    links_path = os.path.join(data_dir, 'links.csv')\n",
        "    if not os.path.isfile(links_path):\n",
        "        if not os.path.isfile(os.path.join(data_dir, 'links.tar.bz2')):\n",
        "            subprocess.run(\n",
        "                \"wget https://downloads.tatoeba.org/exports/links.tar.bz2 -P \" + data_dir,\n",
        "                shell=True)\n",
        "\n",
        "        subprocess.run(\"tar xvjC {0} -f {0}/links.tar.bz2\".format(data_dir), shell=True)\n",
        "\n",
        "    # read all data\n",
        "    sentences = pd.read_csv(sentences_path, names=['id', 'lang', 'text'], header=None, delimiter='\\t')\n",
        "    links = pd.read_csv(links_path, names=['sent_id', 'tran_id'], header=None, delimiter='\\t')\n",
        "\n",
        "    # extract source - target connected\n",
        "    source_sentences = sentences[sentences.lang == source_lang]\n",
        "    source_sentences = source_sentences.merge(links, left_on='id', right_on='sent_id')\n",
        "    target_sentences = sentences[sentences.lang == target_lang]\n",
        "\n",
        "    bilang_sentences = source_sentences.merge(target_sentences, left_on='tran_id',\n",
        "                                              right_on='id',\n",
        "                                              suffixes=[source_lang, target_lang])\n",
        "    bilang_sentences = bilang_sentences[['text' + source_lang, 'text' + target_lang]]\n",
        "\n",
        "    # save results\n",
        "    file_name = os.path.join(data_dir, '{source}-{target}.csv'.format(source=source_lang, target=target_lang))\n",
        "    bilang_sentences.to_csv(file_name, index=False, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3eoCiISHOYk"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "source_lang = 'eng'\n",
        "target_lang = 'tam'\n",
        "data_dir = '/content/data/'\n",
        "\n",
        "os.chdir('../')\n",
        "corpus = pd.read_csv(os.path.join(data_dir, '{}-{}.csv'.format(source_lang, target_lang)), delimiter='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyUft53uMbeR",
        "outputId": "0979b5ec-836f-4a7f-c6a7-62b9c4e96f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texteng</th>\n",
              "      <th>texttam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Most people think I'm crazy.</td>\n",
              "      <td>நிறைய மக்கள் நான் பைத்தியம் என்று எண்ணுகிறார்கள்</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love you.</td>\n",
              "      <td>நான் உன்னை காதலிக்கிறேன்.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Where are you?</td>\n",
              "      <td>நீ எங்கே இருக்கிறாய்?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When did you come to Japan?</td>\n",
              "      <td>நீ எப்பொழுது ஜப்பான் வந்தாய்?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When is your birthday?</td>\n",
              "      <td>உங்கள் பிறந்த நாள் எப்போது ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>I will sleep.</td>\n",
              "      <td>நான் தூங்குவேன்.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>They told me why they needed my help.</td>\n",
              "      <td>அவர்கள் எதனால் என் உதவி தேவைப்பட்டது என்று கூற...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>Tom drank with us until after midnight.</td>\n",
              "      <td>டாம் நள்ளிரவு வரை எங்களுடன் குடித்தார்.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>Tom's car was manufactured in the 1980s.</td>\n",
              "      <td>டாமின் கார் 1980 களில் தயாரிக்கப்பட்டது.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>Three vicious dogs attacked Tom.</td>\n",
              "      <td>மூன்று மோசமான நாய்கள் டாமை தாக்கின</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>317 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      texteng                                            texttam\n",
              "0                Most people think I'm crazy.   நிறைய மக்கள் நான் பைத்தியம் என்று எண்ணுகிறார்கள்\n",
              "1                                 I love you.                          நான் உன்னை காதலிக்கிறேன்.\n",
              "2                              Where are you?                              நீ எங்கே இருக்கிறாய்?\n",
              "3                 When did you come to Japan?                      நீ எப்பொழுது ஜப்பான் வந்தாய்?\n",
              "4                      When is your birthday?                       உங்கள் பிறந்த நாள் எப்போது ?\n",
              "..                                        ...                                                ...\n",
              "312                             I will sleep.                                   நான் தூங்குவேன்.\n",
              "313     They told me why they needed my help.  அவர்கள் எதனால் என் உதவி தேவைப்பட்டது என்று கூற...\n",
              "314   Tom drank with us until after midnight.            டாம் நள்ளிரவு வரை எங்களுடன் குடித்தார்.\n",
              "315  Tom's car was manufactured in the 1980s.           டாமின் கார் 1980 களில் தயாரிக்கப்பட்டது.\n",
              "316          Three vicious dogs attacked Tom.                 மூன்று மோசமான நாய்கள் டாமை தாக்கின\n",
              "\n",
              "[317 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acq3dcEyMfmr"
      },
      "source": [
        "SOS_token = '<start>'\n",
        "EOS_token = '<end>'\n",
        "UNK_token = '<unk>'\n",
        "PAD_token = '<pad>'\n",
        "\n",
        "SOS_idx = 0\n",
        "EOS_idx = 1\n",
        "UNK_idx = 2\n",
        "PAD_idx = 3\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.index2word = {\n",
        "            SOS_idx: SOS_token,\n",
        "            EOS_idx: EOS_token,\n",
        "            UNK_idx: UNK_token,\n",
        "            PAD_idx: PAD_token\n",
        "        }\n",
        "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "\n",
        "    def index_words(self, words):\n",
        "        for word in words:\n",
        "            self.index_word(word)\n",
        "\n",
        "    def index_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            n_words = len(self)\n",
        "            self.word2index[word] = n_words\n",
        "            self.index2word[n_words] = word\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.index2word) == len(self.word2index)\n",
        "        return len(self.index2word)\n",
        "\n",
        "    def unidex_words(self, indices):\n",
        "        return [self.index2word[i] for i in indices]\n",
        "\n",
        "    def to_file(self, filename):\n",
        "        values = [w for w, k in sorted(list(self.word2index.items())[5:])]\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write('\\n'.join(values))\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, filename):\n",
        "        vocab = Vocab()\n",
        "        with open(filename, 'r') as f:\n",
        "            words = [l.strip() for l in f.readlines()]\n",
        "            vocab.index_words(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlzD4DiXMkT4",
        "outputId": "d7e616ac-381d-46da-e264-42fc8b05763c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install inltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: inltk in /usr/local/lib/python3.6/dist-packages (0.9)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: fastai==1.0.57 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.57)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from inltk) (1.1.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from inltk) (0.1.94)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from inltk) (7.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from inltk) (4.6.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from inltk) (2.7.1)\n",
            "Requirement already satisfied: aiohttp>=3.5.4 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.7.2)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from inltk) (20.4)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.18.5)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from inltk) (2.2.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from inltk) (0.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from inltk) (3.7.4.3)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: async-timeout>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from inltk) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (1.3.1+cu100)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (0.4.2+cu100)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2018.9)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.6.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (20.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (5.0.0)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (2.4.7)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (2.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (1.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSvFiLhzOm0e",
        "outputId": "2293d145-eaae-481d-956c-3eae7a9ebdc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.3.1+cu100 in /usr/local/lib/python3.6/dist-packages (1.3.1+cu100)\n",
            "Requirement already satisfied: torchvision==0.4.2+cu100 in /usr/local/lib/python3.6/dist-packages (0.4.2+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1+cu100) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2+cu100) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2+cu100) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAVGsh83NRuo",
        "outputId": "bfd2b91d-ccb3-4bc9-aefd-7b7c002cad11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from inltk.inltk import setup\n",
        "setup('ta')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2b3dd1423cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/inltk/inltk.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(language_code)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_until_complete_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_task\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh0Lmv2rPu_2"
      },
      "source": [
        "from inltk.inltk import tokenize\n",
        "\n",
        "def tamil_tokenize(text):\n",
        "  return tokenize(text, \"ta\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czCJ6fCfOC8f",
        "outputId": "deeafd5f-e887-470f-d4aa-e28e786cef1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "# from tokenize_uk import tokenize_words\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "max_length = 10\n",
        "min_word_count = 1\n",
        "\n",
        "tokenizers = {\n",
        "    'tam': tamil_tokenize,\n",
        "    'eng': nltk.tokenize.WordPunctTokenizer().tokenize\n",
        "}\n",
        "\n",
        "def preprocess_corpus(sents, tokenizer, min_word_count):\n",
        "    n_words = {}\n",
        "\n",
        "    sents_tokenized = []\n",
        "    for sent in sents:\n",
        "        sent_tokenized = [w.lower() for w in tokenizer(sent)]\n",
        "\n",
        "        sents_tokenized.append(sent_tokenized)\n",
        "\n",
        "        for word in sent_tokenized:\n",
        "            if word in n_words:\n",
        "                n_words[word] += 1\n",
        "            else:\n",
        "                n_words[word] = 1\n",
        "\n",
        "    for i, sent_tokenized in enumerate(sents_tokenized):\n",
        "        sent_tokenized = [t if n_words[t] >= min_word_count else UNK_token for t in sent_tokenized]\n",
        "        sents_tokenized[i] = sent_tokenized\n",
        "\n",
        "    return sents_tokenized\n",
        "\n",
        "def read_vocab(sents):\n",
        "    vocab = Vocab()\n",
        "    for sent in sents:\n",
        "        vocab.index_words(sent)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "source_sents = preprocess_corpus(corpus['text' + source_lang], tokenizers[source_lang], min_word_count)\n",
        "target_sents = preprocess_corpus(corpus['text' + target_lang], tokenizers[target_lang], min_word_count)\n",
        "\n",
        "# Using set to remove duplicates\n",
        "source_sents, target_sents = zip(\n",
        "    *sorted({(tuple(s), tuple(t)) for s, t in zip(source_sents, target_sents)\n",
        "              if len(s) <= max_length and len(t) <= max_length})\n",
        ")\n",
        "\n",
        "source_vocab = read_vocab(source_sents)\n",
        "target_vocab = read_vocab(target_sents)\n",
        "\n",
        "target_vocab.to_file(os.path.join(data_dir, '{}.vocab.txt'.format(target_lang)))\n",
        "source_vocab.to_file(os.path.join(data_dir, '{}.vocab.txt'.format(source_lang)))\n",
        "\n",
        "print('Corpus length: {}\\nSource vocabulary size: {}\\nTarget vocabulary size: {}'.format(\n",
        "    len(source_sents), len(source_vocab.word2index), len(target_vocab.word2index)\n",
        "))\n",
        "examples = list(zip(source_sents, target_sents))[80:90]\n",
        "for source, target in examples:\n",
        "    print('Source: \"{}\", target: \"{}\"'.format(' '.join(source), ' '.join(target)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 243\n",
            "Source vocabulary size: 391\n",
            "Target vocabulary size: 648\n",
            "Source: \"i am not a guide .\", target: \"▁நான் ▁ஒரு ▁வழி ▁காட்டி ▁இல்லை\"\n",
            "Source: \"i am not being guided .\", target: \"▁நான் ▁வழி ▁காட்ட ப் ▁பட்டு க் ▁கொண்டு ▁இருக்க ▁ வில்லை\"\n",
            "Source: \"i am not guided .\", target: \"▁நான் ▁வழி ▁காட்ட ப் ▁பட ▁ வில்லை\"\n",
            "Source: \"i am sleeping .\", target: \"▁நான் ▁தூ ங்க ுகிற ேன் .\"\n",
            "Source: \"i arrived ahead of the others .\", target: \"▁மற்றவர் களுக்கு ▁முன்ன ே ▁நான் ▁வந்த ேன்\"\n",
            "Source: \"i ate too much .\", target: \"▁நான் ▁நிறைய ▁சாப்பிட ் டே ன்\"\n",
            "Source: \"i came to buy vegetables .\", target: \"▁நான் ▁காய்கறி ▁வாங்க ▁கடை க்குப் ▁போன ேன்\"\n",
            "Source: \"i didn ' t inform them .\", target: \"▁நான் ▁அவர் களிடம் ▁சொல்ல வில்லை .\"\n",
            "Source: \"i didn ' t let them know .\", target: \"▁நான் ▁அவர் களிடம் ▁தெரிவிக்க வில்லை .\"\n",
            "Source: \"i didn ' t tell them .\", target: \"▁நான் ▁அவர் களிடம் ▁சொல்ல வில்லை .\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojvho3rDQCPk",
        "outputId": "e3c6f65d-1435-4428-c689-762342eb4049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "source_to_target = {}\n",
        "for source, target in zip(source_sents, target_sents):\n",
        "    if source in source_to_target:\n",
        "        source_to_target[source].append(target)\n",
        "    else:\n",
        "        source_to_target[source] = [target]\n",
        "\n",
        "source_sents, target_sents = zip(*source_to_target.items())\n",
        "len(source_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2jiWi3BQgR0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "source_length = len(source_sents)\n",
        "inidices = np.random.permutation(source_length)\n",
        "\n",
        "training_indices = inidices[:int(source_length*0.8)]\n",
        "dev_indices = inidices[int(source_length*0.8):int(source_length*0.86)]\n",
        "test_indices = inidices[int(source_length*0.86):]\n",
        "\n",
        "training_source = [source_sents[i] for i in training_indices]\n",
        "dev_source = [source_sents[i] for i in dev_indices]\n",
        "test_source = [source_sents[i] for i in test_indices]\n",
        "\n",
        "training_target = [target_sents[i] for i in training_indices]\n",
        "dev_target = [target_sents[i] for i in dev_indices]\n",
        "test_target = [target_sents[i] for i in test_indices]\n",
        "\n",
        "# Unwrap training examples\n",
        "training_t = []\n",
        "training_s = []\n",
        "for source, tt in zip(training_source, training_target):\n",
        "    for target in tt:\n",
        "        training_t.append(target)\n",
        "        training_s.append(source)\n",
        "\n",
        "training_source = training_s\n",
        "training_target = training_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF9Z8FfnQwTk",
        "outputId": "08b02f6d-92a4-4861-bc2b-fe22be0999a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "def indexes_from_sentence(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence]\n",
        "\n",
        "def tensor_from_sentence(vocab, sentence, max_seq_length):\n",
        "    indexes = indexes_from_sentence(vocab, sentence)\n",
        "    indexes.append(EOS_idx)\n",
        "    indexes.insert(0, SOS_idx)\n",
        "    # we need to have all sequences the same length to process them in batches\n",
        "    if len(indexes) < max_seq_length:\n",
        "        indexes += [PAD_idx] * (max_seq_length - len(indexes))\n",
        "    tensor = torch.LongTensor(indexes)\n",
        "    return tensor\n",
        "\n",
        "def tensors_from_pair(source_sent, target_sent, max_seq_length):\n",
        "    source_tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
        "    target_tensor = tensor_from_sentence(target_vocab, target_sent, max_seq_length).unsqueeze(1)\n",
        "    return (source_tensor, target_tensor)\n",
        "\n",
        "max_seq_length = max_length + 2  # 2 for EOS_token and SOS_token\n",
        "\n",
        "training = []\n",
        "for source_sent, target_sent in zip(training_source, training_target):\n",
        "    training.append(tensors_from_pair(source_sent, target_sent, max_seq_length))\n",
        "\n",
        "x_training, y_training = zip(*training)\n",
        "x_training = torch.transpose(torch.cat(x_training, dim=-1), 1, 0)\n",
        "y_training = torch.transpose(torch.cat(y_training, dim=-1), 1, 0)\n",
        "torch.save(x_training, os.path.join(data_dir, 'x_training.bin'))\n",
        "torch.save(y_training, os.path.join(data_dir, 'y_training.bin'))\n",
        "\n",
        "x_development = []\n",
        "for source_sent in dev_source:\n",
        "    tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
        "    x_development.append(tensor)\n",
        "\n",
        "x_development = torch.transpose(torch.cat(x_development, dim=-1), 1, 0)\n",
        "torch.save(x_development, os.path.join(data_dir, 'x_development.bin'))\n",
        "\n",
        "x_test = []\n",
        "for source_sent in test_source:\n",
        "    tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
        "    x_test.append(tensor)\n",
        "\n",
        "x_test = torch.transpose(torch.cat(x_test, dim=-1), 1, 0)\n",
        "torch.save(x_test, os.path.join(data_dir, 'x_test.bin'))\n",
        "\n",
        "USE_CUDA = True\n",
        "if USE_CUDA:\n",
        "    x_training = x_training.cuda()\n",
        "    y_training = y_training.cuda()\n",
        "    x_development = x_development.cuda()\n",
        "    x_test = x_test.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-86b6d33ace47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# 2 for EOS_token and SOS_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'max_length' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cNymgCFQ5PV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}